{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning \n",
    "Scale Drives Deep Learning Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An image is represented by a matrix of pixel values\n",
    "- Most common image format is a 3 layer matrix, where each layer represents the Red, Green and Blue (RGB) values of the image.\n",
    "- Each pixel is represented by a vector of 3 numbers ranging from 0 to 255.\n",
    "- Hence, a 64x64 image will have 64x64x3 = 12288 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "- X = [x<sup>(1)</sup>, x<sup>(2)</sup>, ..., x<sup>(m)</sup>]; a matrix of shape (n<sub>x</sub>, m) where m is the number of training examples. X contains the input features of the training examples.\n",
    "- Y = [y<sup>(1)</sup>, y<sup>(2)</sup>, ..., y<sup>(m)</sup>]; a vector of shape (1, m) where m is the number of training examples. Y contains the labels of the training examples.\n",
    "- Hence, a training example is represented by (x<sup>(i)</sup>, y<sup>(i)</sup>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- Logistic regression is a binary classifier. It classifies the input into one of the two classes.\n",
    "- The output of the logistic regression is a number between 0 and 1.\n",
    "- The output of the logistic regression is the probability of the input belonging to class 1.\n",
    "- The output of the logistic regression is calculated using the sigmoid function.\n",
    "- The sigmoid function is defined as:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "- Given an input X, the output of the logistic regression is calculated as:\n",
    "$$\\hat{y} = \\sigma(w^T X + b)$$ \n",
    "-> where w is a vector of shape (n<sub>x</sub>, 1) and b is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss (Error) Function\n",
    "- The loss function is used to measure the accuracy of the model.\n",
    "- The loss function is defined as:\n",
    "$$L(\\hat{y}, y) = - (y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}))$$\n",
    "- If y = 1, then the loss function is defined as, (we want $\\hat{y}$ to be close to 1):\n",
    "$$L(\\hat{y}, y) = - \\log(\\hat{y})$$\n",
    "- If y = 0, then the loss function is defined as, (we want $\\hat{y}$ to be close to 0):\n",
    "$$L(\\hat{y}, y) = - \\log(1 - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "- The cost function is the average of the loss function over all the training examples.\n",
    "- The cost function is defined as:\n",
    "$$J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- Gradient descent is an optimization algorithm used to minimize the cost function.\n",
    "- Gradient descent is used to find the optimal values of w and b.\n",
    "- The algorithm starts with some initial values of w and b.\n",
    "- The algorithm then iteratively updates the values of w and b to minimize the cost function.\n",
    "- The algorithm stops when the cost function converges to a minimum value.\n",
    "- The algorithm is defined as:\n",
    "$$w = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}$$\n",
    "-> where $\\alpha$ is the learning rate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
